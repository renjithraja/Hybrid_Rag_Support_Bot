import sys, os
from dotenv import load_dotenv
import streamlit as st

# Load environment variables from .env (e.g., disabling Chroma telemetry)
load_dotenv()

# Ensure project root is in Python path so imports work when running via Streamlit
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.append(ROOT)

from app.query_service import QueryService

# Configure Streamlit page for a cleaner UI layout
st.set_page_config(page_title="Hybrid RAG Support Bot", layout="wide")

# Instantiate the service that handles retrieval + generation
qs = QueryService()

# -------------- UI LAYOUT -----------------

st.title("ğŸ”§ Hybrid RAG Support Bot")
st.caption("Answers from your Dell Latitude 5400 manual using metadata-aware retrieval.")

# Single input for user query
question = st.text_input("Ask a question from the manual:")

# Trigger RAG pipeline when the user presses Submit
if st.button("Submit") and question:
    # Visual feedback while retrieval + generation run
    with st.spinner("Thinking..."):
        result = qs.answer(question)

    # Display final answer generated by LLM
    st.subheader("ğŸ“˜ Answer")
    st.write(result["answer"])

    # Show metadata used by the retriever (proves metadata filtering is working)
    st.subheader("ğŸ“ Metadata Used")
    st.json(result["metadata"])

    # Display latency of retrieval and generation separately (required by task spec)
    st.subheader("â± Latency")
    st.write(f"Retrieval: {result['retrieval_time']:.3f}s")
    st.write(f"Generation: {result['generation_time']:.3f}s")
