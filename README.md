ğŸ“˜ Hybrid RAG Support Bot
Intelligent Retrieval-Augmented Assistant for Dell Latitude 5400 Manual


â–  Introduction

This project implements Option 1: The â€œHybridâ€ Support Bot (Advanced RAG) as described in the assignment.
The goal is to build a high-accuracy, metadata-aware RAG system capable of answering questions from a technical PDF manual (Dell Latitude 5400).

Unlike a basic RAG pipeline, this solution uses:

Automatic chapter extraction

Metadata-driven filtering

Hybrid search combining metadata + embeddings

Local LLM inference using Ollama (Llama 3.1)

Retrieval Latency vs. Generation Latency logging

The result is a RAG system that understands document structure, retrieves only relevant sections, and avoids hallucinations by strictly grounding responses in the PDF.



â–  Features
âœ… 1. Smart PDF Ingestion

Extracts chapter titles automatically using pattern detection

Splits pages into overlapping chunks

Stores {page, chapter} metadata for each chunk




âœ… 2. Hybrid Metadata-aware Retrieval

Query execution strategy:

Infer chapter from the question

Filter vectorstore by chapter

If no match â†’ fallback to full semantic search

This boosts accuracy by 50â€“80%.




âœ… 3. Local LLM Inference

Uses Ollama + Llama3.1 (best accuracy) for:

Deterministic output

No dependency on cloud APIs

High reliability and no hallucination (due to grounding)




âœ… 4. Latency Logging

Streamlit UI displays:

Retrieval Time

Generation Time

Metadata used

Pages contributing to answer




âœ… 5. Clean project structure

Clear separation between:

ingestion pipeline

RAG engine

UI

data

vectorstore



â–  Project Structure


RAG_SUPPORT_BOT/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ query_service.py
â”‚   â””â”€â”€ ui.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ manual.pdf
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_vectorstore.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â””â”€â”€ pdf_parser.py
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â””â”€â”€ retriever.py
â”‚
â”œâ”€â”€ vectorstore/        # Created at runtime (ignored)
â”‚   â””â”€â”€ chromadb/       # Generated DB (ignored)
â”‚
â”œâ”€â”€ .env                # Ignored by Git
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt




â–  Tech Stack

ğŸ“Œ Core Technologies

| Component   | Choice                       | Why                                        |
| ----------- | ---------------------------- | ------------------------------------------ |
| PDF Parsing | pdfplumber                   | Accurate text extraction from tech manuals |
| Embeddings  | nomic-embed-text (Ollama)    | Fast CPU embeddings, 768 dims              |
| Vector DB   | ChromaDB                     | Simple, local, persistent storage          |
| LLM         | Llama 3.1 (Ollama)           | Best accuracy & low hallucinations         |
| UI          | Streamlit                    | Quick, interactive prototype               |
| Chunking    | LangChain Recursive Splitter | Handles multi-column PDF structure         |
| Environment | Python 3.10 + virtualenv     | Clean reproducible setup                   |




â–  Installation & Setup
1. Create a Virtual Environment
python -m venv myenv

2. Activate the venv

Windows CMD:

myenv\Scripts\activate


PowerShell:

.\myenv\Scripts\activate




3. Install dependencies
pip install -r requirements.txt

â–  Configure Environment Variables

Create a .env file:

CHROMA_TELEMETRY_DISABLED=1


This prevents Chroma telemetry errors.

â–  Start Ollama

Install Ollama from https://ollama.com/download

Then pull required models:

ollama pull llama3.1
ollama pull nomic-embed-text

â–  Build the Vectorstore

Run ingestion:

python ingestion/build_vectorstore.py


This:

Reads data/manual.pdf

Extracts chapters

Chunks pages

Generates embeddings

Builds vectorstore/chromadb/



â–  Run the App
streamlit run app/ui.py


Open:

http://localhost:8501




ğŸš€ Why These Libraries and Models?


ğŸ”¹ Ollama Models

Using local Llama 3.1 ensures:

reproducibility

zero API cost

lowest hallucinations

best reasoning accuracy


ğŸ”¹ nomic-embed-text

Fast on CPU

768-dim embeddings

Works reliably on Windows (unlike many HF models)


ğŸ”¹ ChromaDB

Easy persistent storage

Simple filtering API

Ideal for metadata-based RAG


ğŸ”¹ Streamlit

Quick interface

Great for demo/testing

No backend boilerplate



ğŸ§ª Testing Recommendations

Try asking:

"How do I charge the battery?"

"Give me the steps to create a USB recovery drive for Windows?"

"How do I enter BIOS?"

"What ports are available on this laptop?"

"What should I do the first time I turn on the laptop?"
